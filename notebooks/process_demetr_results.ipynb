{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories as specified in the dataset are different to the paper\n",
    "# This gives the mapping between them\n",
    "with open(\"../configs/demetr/cat_correction.yaml\") as stream:\n",
    "    try:\n",
    "        cat_correction = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map categories to error severity\n",
    "# Severity is as specified in the paper\n",
    "with open(\"../configs/demetr/cat_severity.yaml\") as stream:\n",
    "    try:\n",
    "        cat_severity = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results from DEMETR paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demetr_results = \"../data/demetr_paper_results_tidy.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demetr_df = pd.read_csv(demetr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_baseline = demetr_df[demetr_df.metric == \"Bleu\"]\n",
    "comet_baseline = demetr_df[demetr_df.metric == \"Comet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and sanity check\n",
    "# We should have 35 categories\n",
    "bleu_baseline = bleu_baseline.sort_values(\"category\")\n",
    "print(len(bleu_baseline))\n",
    "\n",
    "comet_baseline = comet_baseline.sort_values(\"category\")\n",
    "print(len(comet_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load M4ST results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4st_res_dir = \"../outputs/demetr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_files = os.listdir(m4st_res_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all files into a single dataframe\n",
    "results_dataframes = []\n",
    "\n",
    "for i in range(len(res_files)):\n",
    "    try:\n",
    "        res_df = pd.read_json(os.path.join(m4st_res_dir, res_files[i]))\n",
    "        metric = res_files[i].split(\"_\")[0]\n",
    "        id = next(c for c in res_files[i].split(\"_\") if \"id\" in c)\n",
    "        cat = int(id.strip(\"id\"))\n",
    "        res_df = res_df.T\n",
    "        res_df[\"metric\"] = metric\n",
    "        res_df[\"sentence_id\"] = res_df.index\n",
    "        res_df[\"category\"] = cat\n",
    "        results_dataframes.append(res_df)\n",
    "    except IsADirectoryError:\n",
    "        pass\n",
    "\n",
    "all_res = pd.concat(results_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct categories to align with the paper\n",
    "all_res[\"category\"] = all_res[\"category\"].replace(cat_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column indicating DEMETR accuracy\n",
    "all_res[\"correct\"] = all_res[\"mt_score\"] > all_res[\"disfluent_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy is reversed for category 35 (reference as translation) so need to adjust that\n",
    "cat_to_rev = all_res.loc[all_res[\"category\"] == 35]\n",
    "cat_to_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_to_rev[\"correct\"] = cat_to_rev[\"mt_score\"] < cat_to_rev[\"disfluent_score\"]\n",
    "cat_to_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign values to original dataframe\n",
    "all_res.loc[all_res[\"category\"] == 35, \"correct\"] = cat_to_rev.correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check result\n",
    "all_res.loc[all_res[\"category\"] == 35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for severity\n",
    "all_res[\"severity\"] = all_res[\"category\"].map(cat_severity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res.to_csv(\"../outputs/demetr/all/all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: error bars\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "by_language = all_res.groupby(\"source_language\")[\"correct\"].mean()\n",
    "axs.plot(by_language, \"x\")\n",
    "plt.xticks(np.arange(10), by_language.index, rotation=45)\n",
    "plt.ylabel(\"DEMETR accuracy (%)\")\n",
    "plt.xlabel(\"Source language\")\n",
    "plt.title(\"Mean performance across all 35 categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "by_severity = all_res.groupby(\"severity\")[\"correct\"].mean()\n",
    "by_severity.plot(kind=\"bar\")\n",
    "plt.xticks(np.arange(4), by_severity.index, rotation=0)\n",
    "plt.ylabel(\"DEMETR accuracy\")\n",
    "plt.xlabel(\"Severity\")\n",
    "plt.title(\"Mean performance for each error type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably skip in report\n",
    "fig, axs = plt.subplots()\n",
    "sev_by_lang = all_res.groupby([\"source_language\", \"severity\"])[\"correct\"].mean()\n",
    "sev_by_lang.unstack().plot(kind=\"bar\", ax=axs)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"DEMETR accuracy\")\n",
    "plt.xlabel(\"Source language\")\n",
    "plt.title(\"Mean performance for each severity level, by language\")\n",
    "plt.legend(loc=\"right\", bbox_to_anchor=(1.25, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy up COMET naming\n",
    "all_res[\"metric\"] = all_res.metric.replace(\n",
    "    {\n",
    "        \"wmt22-comet-da\": \"wmt22-COMET\",\n",
    "        \"wmt22-cometkiwi-da\": \"wmt22-COMETKiwi\",\n",
    "        \"Bleu\": \"BLEU\",\n",
    "        \"BLASER\": \"BLASER-2\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract this as a table\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "sev_by_lang = all_res.groupby([\"source_language\", \"metric\"])[\"correct\"].mean()\n",
    "sev_by_lang.unstack().plot(kind=\"bar\", ax=axs)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"DEMETR accuracy\")\n",
    "plt.xlabel(\"Source language\")\n",
    "plt.title(\"Mean performance for each metric, by language\")\n",
    "plt.legend(loc=\"right\", bbox_to_anchor=(1.4, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_table = sev_by_lang.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "sev_by_lang = all_res.groupby([\"source_language\", \"metric\"])[\"correct\"].mean()\n",
    "sev_by_lang.unstack().drop(columns=[\"BLASER-2\", \"ChrF2\", \"ChrF1\"]).plot(\n",
    "    kind=\"bar\", ax=axs\n",
    ")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"DEMETR accuracy\")\n",
    "plt.xlabel(\"Source language\")\n",
    "plt.title(\"Mean performance for COMET metrics, by language\")\n",
    "plt.legend(loc=\"right\", bbox_to_anchor=(1.4, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "sorted_overall_mean = (\n",
    "    all_res.groupby([\"metric\"])[\"correct\"].mean().sort_values(ascending=False)\n",
    ")\n",
    "axs.plot(sorted_overall_mean, \"x\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.xlabel(\"Metric\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"Mean performance across all languages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_by_category = (\n",
    "    all_res.groupby([\"metric\", \"category\"])[\"correct\"].mean().reset_index()\n",
    ")\n",
    "corr_by_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = corr_by_category.groupby(\"metric\").median().sort_values(by=\"correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "g = sns.boxplot(\n",
    "    corr_by_category,\n",
    "    x=\"metric\",\n",
    "    y=\"correct\",\n",
    "    fill=False,\n",
    "    ax=axs,\n",
    "    width=0.5,\n",
    "    order=grouped.index,\n",
    ")\n",
    "axs.set_xticklabels(rotation=30, labels=axs.get_xticklabels())\n",
    "axs.set_xlabel(\"Metric\")\n",
    "axs.set_ylabel(\"Accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/demetr/plots/metrics-boxplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What trend would be desirable here? Include in report\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "sev_by_lang = all_res.groupby([\"metric\", \"severity\"])[\"correct\"].mean()\n",
    "sev_by_lang.unstack().plot(kind=\"bar\", ax=axs)\n",
    "plt.xticks(rotation=30)\n",
    "plt.ylabel(\"DEMETR accuracy\")\n",
    "plt.xlabel(\"Metric\")\n",
    "# plt.title(\"Mean performance for each severity level by metric\")\n",
    "plt.legend(loc=\"right\", bbox_to_anchor=(1.25, 0.5))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/demetr/plots/demetr-by-severity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "sev_by_lang = all_res.groupby([\"metric\", \"severity\"])[\"correct\"].mean()\n",
    "sev_by_lang.unstack().plot(kind=\"bar\", ax=axs)\n",
    "plt.xticks(rotation=30)\n",
    "plt.ylabel(\"DEMETR accuracy\")\n",
    "plt.xlabel(\"Metric\")\n",
    "# plt.title(\"Mean performance by severity for COMET metrics\")\n",
    "plt.legend(loc=\"right\", bbox_to_anchor=(1.23, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between M4ST and original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4st_blaser = (\n",
    "    all_res[all_res.metric == \"BLASER\"].groupby(\"category\").correct.mean() * 100\n",
    ")\n",
    "m4st_comet = all_res[all_res.metric == \"COMET\"].groupby(\"category\").correct.mean() * 100\n",
    "m4st_bleu = all_res[all_res.metric == \"Bleu\"].groupby(\"category\").correct.mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_paper = np.array(m4st_blaser) - np.array(comet_baseline.accuracy)\n",
    "diff_new = np.array(m4st_blaser) - np.array(m4st_comet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].plot(list(range(1, 36)), diff_paper, \"ko\")\n",
    "axs[0].axhline(0, linestyle=\"--\", c=\"r\")\n",
    "axs[0].set_title(\"BLASER-2 vs. COMET (Baseline)\")\n",
    "axs[0].set_ylabel(\"Difference (BLASER-2 - COMET)\")\n",
    "axs[0].set_xlabel(\"DEMETR category\")\n",
    "\n",
    "# TODO: This plot separated so we BLASER-2 vs the best COMET metric\n",
    "axs[1].plot(list(range(1, 36)), diff_new, \"ko\")\n",
    "axs[1].axhline(0, linestyle=\"--\", c=\"r\")\n",
    "axs[1].set_title(\"BLASER-2 vs. COMET (M4ST)\")\n",
    "axs[1].set_ylabel(\"Difference (BLASER-2 - COMET)\")\n",
    "axs[1].set_xlabel(\"DEMETR category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_paper = np.array(m4st_blaser) - np.array(bleu_baseline.accuracy)\n",
    "diff_new = np.array(m4st_blaser) - np.array(m4st_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].plot(list(range(1, 36)), diff_paper, \"ko\")\n",
    "axs[0].axhline(0, linestyle=\"--\", c=\"r\")\n",
    "axs[0].set_title(\"BLASER-2 vs. Bleu (Baseline)\")\n",
    "axs[0].set_ylabel(\"Difference (BLASER-2 - Bleu)\")\n",
    "axs[0].set_xlabel(\"DEMETR category\")\n",
    "\n",
    "axs[1].plot(list(range(1, 36)), diff_new, \"ko\")\n",
    "axs[1].axhline(0, linestyle=\"--\", c=\"r\")\n",
    "axs[1].set_title(\"BLASER-2 vs. Bleu (M4ST)\")\n",
    "axs[1].set_ylabel(\"Difference (BLASER-2 - Bleu)\")\n",
    "axs[1].set_xlabel(\"DEMETR category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare BLASER to Bleu/COMET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demetr_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demetr_df[\"metric\"] = demetr_df.metric.replace({\"Comet\": \"COMET\", \"ChrF\": \"ChrF1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(all_res.metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(demetr_df.metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset DEMETR paper results to match the metrics I used\n",
    "demetr_df_match = demetr_df[demetr_df.metric.isin(np.unique(all_res.metric))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demetr_df_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4st_by_cat = all_res[[\"metric\", \"category\", \"correct\"]].groupby([\"category\", \"metric\"])\n",
    "m4st_by_cat = m4st_by_cat.correct.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4st_by_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust percentage\n",
    "m4st_by_cat[\"correct\"] = m4st_by_cat[\"correct\"] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for plotting by source\n",
    "demetr_df_match[\"source\"] = \"Karpinska et al.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4st_by_cat[\"source\"] = \"ARC\"\n",
    "m4st_by_cat = m4st_by_cat.rename(columns={\"correct\": \"accuracy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(\n",
    "    demetr_df_match,\n",
    "    m4st_by_cat,\n",
    "    on=[\"category\", \"metric\", \"source\", \"accuracy\"],\n",
    "    how=\"outer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged[merged.metric.isin([\"Bleu\", \"COMET\", \"ChrF1\", \"ChrF2\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    data=merged[merged.category <= 15],\n",
    "    kind=\"bar\",\n",
    "    x=\"category\",\n",
    "    y=\"accuracy\",\n",
    "    hue=\"source\",\n",
    "    errorbar=\"sd\",\n",
    "    palette=\"dark\",\n",
    "    alpha=0.6,\n",
    "    height=6,\n",
    "    aspect=11.7 / 8.27,\n",
    ")\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"Category\", \"Accuracy\")\n",
    "g.legend.set_title(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    data=merged[merged.category > 15],\n",
    "    kind=\"bar\",\n",
    "    x=\"category\",\n",
    "    y=\"accuracy\",\n",
    "    hue=\"source\",\n",
    "    errorbar=\"sd\",\n",
    "    palette=\"dark\",\n",
    "    alpha=0.6,\n",
    "    height=6,\n",
    "    aspect=11.7 / 8.27,\n",
    ")\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"Category\", \"Accuracy\")\n",
    "g.legend.set_title(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLASER only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4st_res_dir = \"../outputs/demetr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blaser_new_15 = pd.read_json(\n",
    "    os.path.join(m4st_res_dir, \"BLASER_REF_minor_id15_case.json\")\n",
    ")\n",
    "blaser_new_8 = pd.read_json(\n",
    "    os.path.join(m4st_res_dir, \"BLASER_Ref_critical_id8_negation.json\")\n",
    ")\n",
    "blaser_new_6 = pd.read_json(\n",
    "    os.path.join(m4st_res_dir, \"BLASER_Ref_critical_id6_addition.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blaser_new_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blaser_new_15 = blaser_new_15.T\n",
    "blaser_new_8 = blaser_new_8.T\n",
    "blaser_new_6 = blaser_new_6.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blaser_new_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(blaser_new_15.source_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blaser_new_15[\"diff\"] = blaser_new_15.mt_score - blaser_new_15.disfluent_score\n",
    "blaser_new_8[\"diff\"] = blaser_new_8.mt_score - blaser_new_8.disfluent_score\n",
    "blaser_new_6[\"diff\"] = blaser_new_6.mt_score - blaser_new_6.disfluent_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blaser_new_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "blaser_new_15.groupby(\"source_language\").mean()[\"diff\"].plot(ax=axs)\n",
    "blaser_new_8.groupby(\"source_language\").mean()[\"diff\"].plot(ax=axs)\n",
    "blaser_new_6.groupby(\"source_language\").mean()[\"diff\"].plot(ax=axs)\n",
    "\n",
    "fig.legend(\n",
    "    labels=[\"Pronoun case\", \"Negation\", \"Addition\"],\n",
    "    loc=\"right\",\n",
    "    bbox_to_anchor=(1.15, 0.5),\n",
    ")\n",
    "axs.set_ylabel(\"Score difference\")\n",
    "plt.xticks(np.arange(10), np.unique(blaser_new_15.source_language), rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
